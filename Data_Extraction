from pathlib import Path
from typing import Dict, List, Optional, Set

import io
import time
import numpy as np
import pandas as pd
import requests

# Try importing BeautifulSoup, throw clean instructions if missing
try:
    from bs4 import BeautifulSoup
except ImportError as e:
    raise ImportError(
        "The 'bs4' package is missing.\n"
        "Install it using:\n"
        "  conda install -c conda-forge beautifulsoup4\n"
        "or\n"
        "  python -m pip install beautifulsoup4\n"
    ) from e

# -------------------------------------------
# CONFIG CONSTANTS
# -------------------------------------------

# Base URL for AirfoilTools website
AIRFOILTOOLS_BASE = "http://airfoiltools.com"

# HTTP headers for all requests to mimic a browser and avoid blocks
HEADERS = {
    "User-Agent": "Mozilla/5.0 (compatible; airfoil-ml-bot/0.1)"
}

# Persistent session to reuse TCP connections and reduce load time
session = requests.Session()
session.headers.update(HEADERS)

# Max number of airfoils to scrape (set None to scrape all)
MAX_AIRFOILS: Optional[int] = 10

# Output path for final ML-ready dataset
OUTPUT_CSV = Path("airfoil_dataset_all_airfoils.csv")


# -------------------------------------------
# SAFE REQUEST WRAPPER
# -------------------------------------------

def safe_get(url: str, params=None, sleep: float = 0.25) -> requests.Response:
    """
    Fetch URL safely with:
    - Small sleep to prevent rate limiting
    - Short timeout to avoid long hangs
    - Raises clean errors for debugging
    """
    time.sleep(sleep)  # Respectful crawl delay
    try:
        resp = session.get(url, params=params, timeout=7)
        resp.raise_for_status()
        return resp
    except requests.exceptions.RequestException as e:
        raise RuntimeError(f"[ERROR] Failed to fetch {url}\nReason: {e}") from e


# -------------------------------------------
# URL NORMALIZER (convert relative URL → absolute)
# -------------------------------------------

def normalize_url(href: str) -> str:
    """
    Convert href into a full absolute URL if it's not already.
    Helps during DFS to avoid broken relative links.
    """
    if href.startswith("http://") or href.startswith("https://"):
        return href  # Already absolute
    if href.startswith("/"):
        return AIRFOILTOOLS_BASE + href  # Root-relative link
    return AIRFOILTOOLS_BASE + "/" + href.lstrip("/")  # Generic relative link


# Build URL for specific airfoil detail lookup
def build_airfoil_details_url(airfoil_id: str) -> str:
    return f"{AIRFOILTOOLS_BASE}/airfoil/details?airfoil={airfoil_id}"

# Build URL for executing polar → CSV export
def build_polar_csv_url(polar_id: str) -> str:
    return f"{AIRFOILTOOLS_BASE}/polar/csv?polar={polar_id}"


# -------------------------------------------
# DISCOVER AIRFOIL IDs FROM DATABASE LIST
# -------------------------------------------

def discover_all_airfoil_ids() -> List[str]:
    """
    Crawl "search/airfoils" pages to collect all airfoil IDs.

    Approach:
    1. Start from /search/airfoils
    2. DFS through any search/airfoils* links
    3. Extract airfoil ID from links matching: airfoil/details?airfoil=<id>
    """
    start_url = f"{AIRFOILTOOLS_BASE}/search/airfoils"
    to_visit: List[str] = [start_url]  # Stack for DFS crawl
    visited_pages: Set[str] = set()    # Track visited list pages
    airfoil_ids: Set[str] = set()      # Store discovered unique airfoil IDs

    print(f"[INFO] Starting crawl from: {start_url}")

    while to_visit:
        url = to_visit.pop()  # DFS pop
        if url in visited_pages:
            continue  # Skip if already visited
        visited_pages.add(url)

        print(f"[INFO] Scraping airfoil list page: {url}")

        try:
            resp = safe_get(url)
        except Exception as e:
            print(f"[WARN] Could not fetch airfoil list page: {e}")
            continue  # Continue crawl even if page fails

        soup = BeautifulSoup(resp.text, "html.parser")

        # Loop through all HTML links on the page
        for a in soup.find_all("a", href=True):
            href = a["href"]

            # Case 1: If link contains airfoil/details?airfoil=XYZ, extract XYZ
            if "airfoil/details?airfoil=" in href:
                part = href.split("airfoil=")[-1]  # Get everything after airfoil=
                part = part.split("&")[0]         # Clip after & if present
                airfoil_id = part.strip()
                if airfoil_id:
                    airfoil_ids.add(airfoil_id)

            # Case 2: If link contains search/airfoils, queue it for DFS crawl
            if "search/airfoils" in href:
                next_url = normalize_url(href)
                if next_url not in visited_pages:
                    to_visit.append(next_url)

    airfoil_list = sorted(airfoil_ids)
    print(f"[INFO] Finished crawl. Found {len(airfoil_list)} airfoils.")
    return airfoil_list


# -------------------------------------------
# FETCH AIRFOIL COORDINATES (.dat)
# -------------------------------------------

def find_coords_dat_url(airfoil_id: str) -> str:
    """
    Open the airfoil details page and look for coordinate file ending in .dat
    or links containing the keyword "coord"
    """
    details_url = build_airfoil_details_url(airfoil_id)
    resp = safe_get(details_url)
    soup = BeautifulSoup(resp.text, "html.parser")

    candidates: List[str] = []

    # Look for all links on the page that may contain coordinate data
    for a in soup.find_all("a", href=True):
        href = a["href"]
        if href.endswith(".dat") or "coord" in href.lower():
            candidates.append(href)

    candidates = list(set(candidates))  # Remove duplicates

    if not candidates:
        raise RuntimeError(f"No coordinate file found for {airfoil_id}")

    # Prefer .dat if possible
    chosen = next((c for c in candidates if c.endswith(".dat")), None)
    if chosen is None:
        chosen = candidates[0]  # Otherwise fallback to first result

    dat_url = normalize_url(chosen)
    print(f"[INFO] Coordinate file for {airfoil_id}: {dat_url}")
    return dat_url


def parse_coords_text_to_df(text: str) -> pd.DataFrame:
    """
    Convert raw coords file text into DataFrame with columns ['x', 'y'].
    Skips comment lines and malformed rows.
    """
    xs = []
    ys = []

    # Read numeric values from coord file
    for line in text.splitlines():
        line = line.strip()
        if not line or line.startswith("#"):
            continue  # Skip comments / blank lines

        parts = line.split()
        if len(parts) < 2:
            continue  # Skip invalid lines

        try:
            x = float(parts[0])
            y = float(parts[1])
        except ValueError:
            continue  # Skip non-numeric lines

        xs.append(x)
        ys.append(y)

    if not xs:
        raise RuntimeError("[ERROR] No valid coordinate points parsed.")

    return pd.DataFrame({"x": xs, "y": ys})


def fetch_airfoil_coords(airfoil_id: str) -> Optional[pd.DataFrame]:
    """
    Wrapper to:
    1. Find coordinate file URL
    2. Fetch it
    3. Parse it to DataFrame
    Returns None if any step fails, so airfoil can be skipped instead of crashing.
    """
    try:
        dat_url = find_coords_dat_url(airfoil_id)
        resp = safe_get(dat_url)
        text = resp.text
        df = parse_coords_text_to_df(text)
        df["airfoil_id"] = airfoil_id  # Tag coords with airfoil ID
        return df
    except Exception as e:
        print(f"[WARN] Skipping {airfoil_id} (coords failed): {e}")
        return None  # Skip this airfoil on failure


# -------------------------------------------
# FETCH POLAR DATA + PARSE
# -------------------------------------------

def discover_polar_ids_for_airfoil(airfoil_id: str) -> List[str]:
    """
    Find all polar IDs attached to an airfoil.
    Example: xf-naca2412-il-100000
    """
    url = build_airfoil_details_url(airfoil_id)
    resp = safe_get(url)
    soup = BeautifulSoup(resp.text, "html.parser")

    polar_ids = []

    # Collect polar ID from links matching polar/details?polar=XYZ
    for a in soup.find_all("a", href=True):
        href = a["href"]
        if "polar/details?polar=" in href:
            part = href.split("polar=")[-1]
            part = part.split("&")[0]
            polar_ids.append(part)

    return sorted(set(polar_ids))  # Remove duplicates + sort


def parse_polar_text_to_df(text: str) -> pd.DataFrame:
    """
    Convert the polar CSV text into a structured DataFrame.

    Method:
    - Ignore headers/comments
    - Convert commas to spaces
    - Keep rows where first 2 values are numeric → actual data
    """
    rows = []
    for line in text.splitlines():
        line = line.strip()
        if not line or line.startswith("#") or "polar" in line.lower():
            continue  # Skip headers/comments

        parts = line.replace(",", " ").split()
        if len(parts) < 2:
            continue  # Skip malformed lines

        # Check if first 2 values are numeric, else skip
        try:
            float(parts[0]), float(parts[1])
        except ValueError:
            continue

        rows.append(parts)

    if not rows:
        raise RuntimeError("[ERROR] No valid polar rows parsed.")

    # Convert to float array
    ncols = len(rows[0])
    rows = [r[:ncols] for r in rows]
    arr = np.array(rows, dtype=float)

    # Expected key columns
    base_cols = ["alpha", "cl", "cd", "cdp", "cm", "top_xtr", "bot_xtr"]
    cols = base_cols[:ncols]

    df = pd.DataFrame(arr, columns=cols)
    print(f"[DEBUG] Parsed polar data with {len(cols)} columns.")
    return df


def fetch_single_polar(polar_id: str) -> pd.DataFrame:
    """
    Download CSV polar data for one polar ID, and attach the polar ID to the row.
    """
    csv_url = build_polar_csv_url(polar_id)
    resp = safe_get(csv_url)
    text = resp.text

    df = parse_polar_text_to_df(text)
    df["polar_id"] = polar_id  # Label the polar source
    return df


# -------------------------------------------
# EXTRACT GEOMETRY METRICS FROM COORDS
# -------------------------------------------

def compute_geometry_features(coords_df: pd.DataFrame) -> Dict[str, float]:
    """
    Compute geometry-based ML features including:
    - Max thickness + position
    - Camber + position along chord
    """

    df = coords_df[["x", "y"]].copy()
    df = df.sort_values("x")

    # Round X values to group top/bottom surface points together
    df["x_round"] = df["x"].round(3)
    grouped = df.groupby("x_round")

    # Approximate surfaces
    upper = grouped["y"].max()
    lower = grouped["y"].min()

    x_vals = upper.index.to_numpy(dtype=float)
    thickness = (upper - lower).to_numpy()
    camber = ((upper + lower) / 2.0).to_numpy()

    # Extract peak thickness
    idx_tmax = np.argmax(thickness)
    max_thickness = float(thickness[idx_tmax])
    x_at_max_thickness = float(x_vals[idx_tmax])

    # Extract peak camber
    idx_cmax = np.argmax(np.abs(camber))
    max_camber = float(camber[idx_cmax])
    x_at_max_camber = float(x_vals[idx_cmax])

    return {
        "max_thickness": max_thickness,
        "x_at_max_thickness": x_at_max_thickness,
        "max_camber": max_camber,
        "x_at_max_camber": x_at_max_camber,
    }


# -------------------------------------------
# REYNOLDS EXTRACTION FROM POLAR ID
# -------------------------------------------

def parse_reynolds_from_polar_id(polar_id: str) -> Optional[float]:
    """
    Extract Reynolds number from polar ID if numeric chunk exists.
    """
    parts = polar_id.split("-")
    nums = [p for p in parts if p.isdigit()]
    if not nums:
        return None  # No Reynolds tagged
    return float(nums[-1])  # Last numeric chunk is usually Re


# -------------------------------------------
# SUMMARIZE LIFT SLOPE + MAX CL
# -------------------------------------------

def summarize_single_polar(polar_df: pd.DataFrame) -> Dict[str, float]:
    """
    Generate ML-relevant aerodynamic summary stats:
    - Max Cl
    - Alpha at max Cl
    - Lift slope dCl/dAlpha in linear region (least squares fit)
    """

    # Validate required columns exist
    if "alpha" not in polar_df.columns or "cl" not in polar_df.columns:
        raise ValueError("Polar data must include alpha and cl")

    alpha = pd.to_numeric(polar_df["alpha"], errors="coerce")
    cl = pd.to_numeric(polar_df["cl"], errors="coerce")

    mask = alpha.notna() & cl.notna()
    alpha = alpha[mask]
    cl = cl[mask]

    if cl.empty:
        raise ValueError("[ERROR] No numeric lift coefficient values found.")

    # Get max Cl
    idx_max = cl.idxmax()
    max_cl = float(cl.loc[idx_max])
    alpha_at_max_cl = float(alpha.loc[idx_max])

    # Fit linear lift slope if possible
    lin_mask = (alpha >= -5.0) & (alpha <= 10.0)
    alpha_lin = alpha[lin_mask]
    cl_lin = cl[lin_mask]

    if len(alpha_lin) >= 3:
        A = np.vstack([alpha_lin.to_numpy(), np.ones_like(alpha_lin.to_numpy())]).T
        m, b = np.linalg.lstsq(A, cl_lin.to_numpy(), rcond=None)[0]
        cl_alpha_slope = float(m)
    else:
        cl_alpha_slope = float("nan")  # Not enough points

    return {
        "max_cl": max_cl,
        "alpha_at_max_cl": alpha_at_max_cl,
        "cl_alpha_slope": cl_alpha_slope,
    }


# -------------------------------------------
# BUILD FULL ML DATASET
# -------------------------------------------

def build_dataset(airfoil_ids: List[str]) -> pd.DataFrame:
    """
    For each airfoil:
    1. Download coords and compute geometry features
    2. Discover + download polars
    3. Keep only rows where alpha > 0 (forward AoA region)
    4. Attach geometry features to each polar row
    """

    all_rows = []

    for i, airfoil_id in enumerate(airfoil_ids, start=1):
        print(f"[INFO] → Processing airfoil ({i}): {airfoil_id}")

        # Fetch geometry
        coords_df = fetch_airfoil_coords(airfoil_id)
        if coords_df is None:
            continue  # Skip if geometry missing

        # Compute geometry features
        try:
            geom_feats = compute_geometry_features(coords_df)
        except Exception:
            continue  # Skip if geometry calc fails

        # Discover polars
        try:
            polar_df = fetch_all_polars_for_airfoil(airfoil_id, max_polars=None)
        except Exception:
            continue  # Skip if polars fail

        polar_df = polar_df.copy()
        polar_df["alpha"] = pd.to_numeric(polar_df["alpha"], errors="coerce")

        mask = polar_df["alpha"].notna() & (polar_df["alpha"] > 0.0)
        polar_df = polar_df[mask]

        if polar_df.empty:
            continue  # Skip if no positive AoA data points

        # Build rows
        for _, r in polar_df.iterrows():
            Re = parse_reynolds_from_polar_id(r["polar_id"])
            row = {
                "airfoil_id": airfoil_id,
                "polar_id": r["polar_id"],
                "Re": Re,
                "alpha": float(r["alpha"]),
                "cl": float(r.get("cl", np.nan)),
                "cd": float(r.get("cd", np.nan)),
                "cdp": float(r.get("cdp", np.nan)),
                "cm": float(r.get("cm", np.nan)),
                "top_xtr": float(r.get("top_xtr", np.nan)),
                "bot_xtr": float(r.get("bot_xtr", np.nan)),
            }

            # Attach geometry features
            row.update(geom_feats)
            all_rows.append(row)

    return pd.DataFrame(all_rows)


# -------------------------------------------
# MAIN EXEC RUN
# -------------------------------------------

def main():
    # Step 1: Discover airfoil list
    airfoil_ids = discover_all_airfoil_ids()

    # Optional limit for fast debug/testing runs
    if MAX_AIRFOILS is not None:
        airfoil_ids = airfoil_ids[:MAX_AIRFOILS]

    print(f"[INFO] Will scrape N={len(airfoil_ids)} airfoils")

    # Step 2: Compile ML dataset
    df = build_dataset(airfoil_ids)

    # Ensure parent folder exists, then save to CSV
    OUTPUT_CSV.parent.mkdir(exist_ok=True, parents=True)
    df.to_csv(OUTPUT_CSV, index=False)

    print(f"[INFO] ✅ Dataset saved: {OUTPUT_CSV.resolve()}")


if __name__ == "__main__":
    main()
